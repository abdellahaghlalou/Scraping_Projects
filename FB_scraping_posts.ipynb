{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import maskpass\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacebookPost():\n",
    "    POST_LOGIN_URL=\"https://m.facebook.com/login.php?refsrc=https%3A%2F%2Fm.facebook.com%2F&refid=8\"\n",
    "    def login():\n",
    "        \"\"\"\n",
    "        Login to Facebook\n",
    "        \n",
    "        \"\"\"\n",
    "        email=str(input(\"Email : \"))\n",
    "        password=str(input(\"password : \"))\n",
    "        login={\n",
    "            'email':email,\n",
    "            'pass':password\n",
    "            }\n",
    "        return login\n",
    "    \n",
    "    payload=login() \n",
    "    \n",
    "        \n",
    "    def parse_html(self,request_url,session):\n",
    "     \n",
    "        post=session.post(self.POST_LOGIN_URL,data=self.payload) \n",
    "        page =session.get(request_url)\n",
    "        return page\n",
    "\n",
    "\n",
    "    def Soup(self,page):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        soup : Beautiful Soup object \n",
    "        \"\"\"\n",
    "  \n",
    "        soup=BeautifulSoup(page.content,\"html.parser\")\n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    def get_page(self,request_url,session):\n",
    "\n",
    "        page=session.get(request_url) \n",
    "        soup=self.Soup(page)\n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    \n",
    "    def url_Keyword(self,key):\n",
    "        \"\"\"\n",
    "        search for the post key using mbasic.facebook.com\n",
    "        \n",
    "        \"\"\"\n",
    "        url=key.replace('www','mbasic')\n",
    "        #url=key.replace(\"%20\",\"+\")\n",
    "        \n",
    "        return url\n",
    "    \n",
    "    \n",
    "    \n",
    "    def clean_url(self,Urls_list,URLS):\n",
    "        \"\"\"\n",
    "        Clean post URL\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(len(Urls_list)):\n",
    "            \n",
    "            #www.facebook.com\n",
    "            f_url=Urls_list[i].replace('https://m.facebook.com','')\n",
    "            f_url=f_url.replace(\"&__tn__=%2As\",'')\n",
    "            f_url=f_url.replace(\"&__tn__=%2AW\",'')\n",
    "            f_url=f_url.replace(\"&__tn__=EH\",'')\n",
    "            f_url=f_url.replace(\"?__tn__=%2AW\",'')            \n",
    "            f_url=f_url.replace(\"?#footer_action_list\",'')\n",
    "            f_url=f_url.replace(\"#footer_action_list\",'')\n",
    "            URLS.append('https://www.facebook.com'+ f_url)\n",
    "            \n",
    "            #mbasic urls:\n",
    "            Urls_list[i]=Urls_list[i].replace('https://m.facebook.com','')\n",
    "            Urls_list[i]='https://mbasic.facebook.com'+ Urls_list[i]\n",
    "        \n",
    "        return Urls_list,URLS\n",
    "    \n",
    "   \n",
    "    \n",
    "    def get_images(self,soup,Images):\n",
    "        \"\"\"\n",
    "        Extract and Clean Images URLs\n",
    "        \"\"\"\n",
    "        \n",
    "        img=soup.find_all('a',href=re.compile(\"/photo.php?fbid=\"))\n",
    "        img1=soup.find_all('a',href=re.compile(\"/photo\"))\n",
    "        m=' '\n",
    "        if img !=[]:\n",
    "            img_href='https://www.facebook.com'+img[0]['href']\n",
    "            m+=img_href+'\\n'\n",
    "            \n",
    "        elif img1 !=[]:\n",
    "            img_href='https://www.facebook.com'+img1[0]['href']\n",
    "            m+=img_href+'\\n'\n",
    "            \n",
    "        else:\n",
    "            img=soup.find_all('a',href=re.compile(\"pcb\"))\n",
    "            if img !=[]:\n",
    "                for i in img:\n",
    "                    img_href='https://www.facebook.com'+i['href']\n",
    "                    m+=img_href+'\\n'    \n",
    "    \n",
    "            \n",
    "            else:\n",
    "                img=soup.find_all('a',href=re.compile(\"photos\"))\n",
    "                if img !=[]:\n",
    "                    for i in img:\n",
    "                        img_href='https://www.facebook.com'+i['href']\n",
    "                        m+=img_href+'\\n'\n",
    "                                                 \n",
    "        Images.append(m)\n",
    "        \n",
    "        return Images\n",
    "      \n",
    "    \n",
    "    \n",
    "    def posts_info(self,soup,Urls_list,Likes,URLS,Date):\n",
    "        \"\"\"\n",
    "        This function returns the posts urls list  and also the posts descriptions\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        while 1:\n",
    "            time.sleep(0.2)\n",
    "            post=soup.find_all('div',class_=\"by\")        \n",
    "            for i in post:\n",
    "                l=i.find('span',id=re.compile(\"like_\"))\n",
    "                Hr=i.find('a',href=re.compile(\"#footer_action_list\"))\n",
    "                if Hr==None:\n",
    "                    Hr=i.find('a',href=re.compile(\"/story.php\"))\n",
    "                   \n",
    "                        \n",
    "                d=i.find('abbr')\n",
    "                \n",
    "                if Hr!=None:\n",
    "                    Href=Hr['href']\n",
    "                    Href=Href.replace('https://m.facebook.com','')\n",
    "                    Href=Href.replace('https://mbasic.facebook.com','')                    \n",
    "                    Urls_list.append(Href)\n",
    "                    if d !=None:\n",
    "                        date=d.get_text()\n",
    "                        Date.append(date)\n",
    "                    else:\n",
    "                        Date.append('None')\n",
    "   \n",
    "                    if l!=None:    \n",
    "                        if l.get_text()!=None:\n",
    "                            likes=l.get_text()\n",
    "                            if likes==\"Like · React\":\n",
    "                                likes='0'\n",
    "                            else:\n",
    "                                likes=likes.replace('· Like · React','')                                \n",
    "                                likes=likes.replace(\"· Like\",'')\n",
    "                                likes=likes.replace(\"· Love\",'')\n",
    "                                likes=likes.replace(\"· Haha\",'')\n",
    "                                likes=likes.replace(\"· Care\",'')\n",
    "                                likes=likes.replace(\"· Wow\",'')\n",
    "                                likes=likes.replace(\"· Angry\",'')\n",
    "                            Likes.append(likes)\n",
    "                        else:\n",
    "                           Likes.append(\"0\")\n",
    "                    else:\n",
    "                        Likes.append(\"0\")\n",
    "           \n",
    "                        \n",
    "            more=self.more_page(soup)\n",
    "            if more !=None:\n",
    "                soup=self.get_page(more,session)\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "                        \n",
    "        Urls_list,URLS=self.clean_url(Urls_list,URLS)            \n",
    "                \n",
    "        return Urls_list,URLS,Likes,Date\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_profile(self,soup,Name,Profile_url):\n",
    "        \"\"\"\n",
    "        Extract Profiles (Name and URLs)\n",
    "        \"\"\"\n",
    "        h3=soup.find(\"h3\")\n",
    "        \n",
    "        if h3 !=None:\n",
    "            if h3.find(\"a\")!=None:           \n",
    "                name=h3.a.get_text()\n",
    "                if h3.a.has_attr('href') :\n",
    "                    h3_a_tag=h3.a['href']\n",
    "                    h3_a_tag=h3_a_tag.replace(\"&__tn__=C-R\",'')\n",
    "                                        \n",
    "                    profile_url='https://www.facebook.com'+h3_a_tag\n",
    "                else :\n",
    "                    profile_url=\"None\"\n",
    "            \n",
    "        else :\n",
    "            h31=soup.find('a',class_=\"actor-link\")\n",
    "            if h31!=None:\n",
    "                name=h31.get_text()\n",
    "                if h31.has_attr('href'):\n",
    "                    profile_url='https://www.facebook.com'+h31['href']\n",
    "                else:    \n",
    "                    profile_url=\"None\"\n",
    "            else:\n",
    " \n",
    "                name=\"None\"\n",
    "                profile_url=\"None\"\n",
    "       \n",
    "            \n",
    "        Name.append(name)\n",
    "        Profile_url.append(profile_url)\n",
    "        \n",
    "        return Name,Profile_url\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_description(self,soup,descreption):\n",
    "        \"\"\"\n",
    "        Extract Post description\n",
    "        \n",
    "        \"\"\"\n",
    "     \n",
    "        p=soup.findAll(\"p\")\n",
    "            \n",
    "        if p!=[]:\n",
    "            s=' ' \n",
    "            for i in p:\n",
    "                s+=i.get_text()+' '\n",
    "            description.append(s)     \n",
    "        else :\n",
    "            s=' '\n",
    "            h11=soup.find('div',{'data-ft':'{\"tn\":\"*s\"}'})\n",
    "            if h11!=None:\n",
    "                s+=h11.get_text()\n",
    "                description.append(s)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                h12=soup.find('div',{'data-ft':'{\"tn\":\",g\"}'})\n",
    "                if h12!=None:\n",
    "                    s+=h12.get_text().split(\" · in Timeline\")[0].replace('· Public','')\n",
    "                    description.append(s)\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    description.append(\"None\")\n",
    "                \n",
    "        return description  \n",
    "    \n",
    "    \n",
    "         \n",
    "    \n",
    "    def more_page(self,soup):\n",
    "        \"\"\"\n",
    "        returns : url of the next search page\n",
    "        \n",
    "        \"\"\"\n",
    "        if soup.find('div',id='see_more_pager')!=None:\n",
    "            more=soup.find('div',id='see_more_pager').find('a',href=True)['href']\n",
    "        else:\n",
    "            more=None\n",
    "        return more\n",
    "       \n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_reactions(self,Post_soup,session):\n",
    "        \"\"\"\n",
    "        get_reactions returns a list of the psot reactions\n",
    "        \"\"\"\n",
    "        React=['Like', 'Angry', 'Love', 'Haha', 'Sad','Care', 'Wow']        \n",
    "        Nbr_Reactions=['0' for i in range(7)]\n",
    "        Reactions_url_tag=Post_soup.find('a',href=re.compile('/ufi/reaction/profile/'))\n",
    "        \n",
    "        \n",
    "        if Reactions_url_tag != None:\n",
    "            \n",
    "            Reactions_url=\"https://mbasic.facebook.com/\"+Reactions_url_tag['href']\n",
    "            \n",
    "            #Beautiful Soup Object:\n",
    "            Reaction_soup=FP.get_page(Reactions_url,session)\n",
    "            \n",
    "            #Extract the class of Reactions:\n",
    "            React_a=Reaction_soup.findAll('a',class_=\"u\")\n",
    "            for i in React_a:\n",
    "                React_img=i.find('img')\n",
    "                if React_img !=None:\n",
    "                    if React_img.has_attr('alt'):\n",
    "                        R_alt=React_img['alt']\n",
    "                                        \n",
    "                        for j in range(len(React)):\n",
    "                            if R_alt==React[j]:\n",
    "                               \n",
    "                                Nbr_Reactions[j]=i.get_text()\n",
    "                                \n",
    "                                                                                            \n",
    "        return  Nbr_Reactions   \n",
    "    \n",
    "        \n",
    "    \n",
    "    def more_comments(self,soup):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        more_comments \n",
    "        \"\"\"\n",
    "        if soup.find('div',id=re.compile(\"see_next_\"))!=None:\n",
    "            more_comments=soup.find('div',id=re.compile(\"see_next_\")).find('a',href=True)['href'] \n",
    "            more_comments=more_comments.replace('https://m.facebook.com','')\n",
    "        else:\n",
    "            more_comments=None\n",
    "        return more_comments\n",
    "    \n",
    "            \n",
    "    def get_comments(self,soup,d_comments,d_profiles,comments_limit):\n",
    "        \n",
    "        nbr=0\n",
    "        while nbr<=comments_limit: #the maximum value of comments we want to scrap\n",
    "        #Extraction of who comments in this post :\n",
    "            \n",
    "            CommentsTag=soup.find_all('h3')\n",
    "            profile_comments,Name=[],[]\n",
    "            \n",
    "            if CommentsTag!=[]:\n",
    "                for i in CommentsTag:\n",
    "                    a_tag=i.find('a')\n",
    "                    if a_tag !=None:\n",
    "                        if a_tag.has_attr('href'):\n",
    "                            a_href=a_tag['href']\n",
    "                            if (\"refid=52&__tn__=R\" in a_href) or('refid=18&__tn__=R' in a_href)or(\"?rc=p&__tn__=R\" in a_href) :\n",
    "                                a_href=a_href.replace(\"&refid=52&__tn__=R\",'')\n",
    "                                a_href=a_href.replace(\"refid=52&__tn__=R\",'')\n",
    "                                a_href=a_href.replace(\"&refid=18&__tn__=R\",'')\n",
    "                                a_href=a_href.replace(\"?refid=18&__tn__=R\",'')                                                                \n",
    "                                a_href_url='https://www.facebook.com'+a_href\n",
    "                                profile_comments.append(a_href_url)\n",
    "                                Name.append(a_tag.get_text())    \n",
    "                \n",
    "            #Extraction of comments :\n",
    "                                \n",
    "            div=soup.find_all('div')\n",
    "            div_text=[i.get_text() for i in div]\n",
    "            \n",
    "            aa,aa1=[],[]\n",
    "            for i in div_text:\n",
    "                if i not in aa:\n",
    "                    aa.append(i)\n",
    "            for j in aa:\n",
    "                if 'Like · React · Reply · More ·' in j and 'View more comments…' not in j: \n",
    "                    aa1.append(j)\n",
    "                    \n",
    "            ll=[' ' for i in range(len(Name))]\n",
    "            if Name !=[]:\n",
    "                for i in range(len(Name)):\n",
    "                    for j in aa1:\n",
    "                        if Name[i] in j:\n",
    "                            com=j.split(Name[i])[1]\n",
    "                            ll[i]=com.split(\"Like\")[0].replace('\"','')\n",
    "                            if 'Edited ·' in ll[i]:\n",
    "                                ll[i]=com.split(\"Edited ·\")[0]\n",
    "               \n",
    "                    for i in range(len(Name)):\n",
    "                        \n",
    "                        d_comments[Name[i]]=ll[i]            \n",
    "                        d_profiles[Name[i]]=profile_comments[i]\n",
    "            nbr=len(d_comments.keys())\n",
    "            \n",
    "            more=self.more_comments(soup)\n",
    "            if more !=None:\n",
    "                more='https://mbasic.facebook.com'+ more\n",
    "                soup=self.get_page(more,session)\n",
    "                \n",
    "            else:\n",
    "                break     \n",
    "            \n",
    "        \n",
    "        d_comments=json.dumps(d_comments, ensure_ascii=False).encode('utf8').decode()\n",
    "        d_profiles=json.dumps(d_profiles, ensure_ascii=False).encode('utf8').decode()\n",
    "        return d_comments,d_profiles\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    def LoginErr(self,page):\n",
    "        if page.status_code != 200:\n",
    "            return sys.exit(\"\\n 1-The email or password that you've entered is incorrect OR This keyword cannot find any post :(\\n2-Maybe Facebook is asking to conform your identity(cheek you account and change your IP adress).\")\n",
    "       \n",
    "        else :\n",
    "            return \"--------------------------------\\n|         Welcome to           |\\n|    Facebook Scraping Tool    |\\n--------------------------------\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP=FacebookPost() # create a new object of FacebookPost class\n",
    "   \n",
    "session=rq.Session()\n",
    "adapter = rq.adapters.HTTPAdapter(max_retries=20)\n",
    "session.mount('https://', adapter)\n",
    "session.mount('http://', adapter)\n",
    "\n",
    "#OUTPUTS\n",
    "Urls_list,description,ALL_Reactions,Name,Profile_url,URLS,Images,Date,Scraping_Date=([] for i in range(9))\n",
    "Like,Angry,Love,Haha,Sad,Care,Wow=([] for i in range(7))#Reactions\n",
    "Comments,Who_Comment=[],[] #Comments\n",
    "\n",
    "#User intput:\n",
    "file_name=str(input(\"Enter the name you want for the new output file (Maybe the keyword):\"))    \n",
    "page_name=str(input(\"Enter the name of group/page\")) \n",
    "\n",
    "key=\"https://www.facebook.com/search/posts?q=\"+page_name\n",
    "\n",
    "NB_posts=int(input(\"How many posts do you want to scrape :\"))\n",
    "\n",
    "comments_limit=int(input(\"How many comments do you want to scrape :\"))\n",
    "#get the search page :  \n",
    "Request_URL=FP.url_Keyword(key)\n",
    "page=FP.parse_html(Request_URL,session)    \n",
    "    \n",
    "#Test if the page is successfully loaded :\n",
    "a=FP.LoginErr(page) \n",
    "print(a)\n",
    "   \n",
    "\n",
    "#Beautiful Soup Object:    \n",
    "soup=FP.Soup(page)  \n",
    "   \n",
    "print(\"\\nPLEASE WAIT... THIS PROCESS WILL TAKE SOME TIME\") \n",
    "\n",
    "\n",
    "#Start the Extraction:\n",
    "\n",
    "#first part \n",
    "\n",
    "Urls_list,URLS,ALL_Reactions,Date=FP.posts_info(soup,Urls_list,ALL_Reactions,URLS,Date)\n",
    "    \n",
    "print(\"\\n\\nNumber of posts availible for this keyword : \",len(Urls_list))   \n",
    "\n",
    "print('------------------------------------------------------------')\n",
    "print('                  WEB SCRAPING PROCESS')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "#second part \n",
    "if NB_posts>len(Urls_list):\n",
    "    NB_posts=len(Urls_list)\n",
    "    \n",
    "    \n",
    "for i in range(NB_posts):\n",
    "    \n",
    "    url=Urls_list[i]\n",
    "    Post_soup=FP.get_page(url,session)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    Name,Profile_url=FP.get_profile(Post_soup,Name,Profile_url)    \n",
    "    description=FP.get_description(Post_soup,description)\n",
    "    Images=FP.get_images(Post_soup,Images)\n",
    "    now = datetime.now()\n",
    "    dt= now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    Scraping_Date.append(dt)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    like,angry,love,haha,sad,care,wow=FP.get_reactions(Post_soup,session)\n",
    "    Like.append(like)\n",
    "    Angry.append(angry)\n",
    "    Love.append(love)\n",
    "    Haha.append(haha)\n",
    "    Sad.append(sad)\n",
    "    Care.append(care)\n",
    "    Wow.append(wow)\n",
    "    \n",
    "    d_comments,d_profiles={},{}\n",
    "    C,W=FP.get_comments(Post_soup,d_comments,d_profiles,comments_limit)  \n",
    "    Who_Comment.append(W)\n",
    "    Comments.append(C)\n",
    "    \n",
    "    print('Post{} successfully scraped :) ...'.format(i+1))\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    \n",
    "print(\"-------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name=[i  for i in Name if i == page_name]\n",
    "NB_posts=len(Name)\n",
    "df1=pd.DataFrame({\"Name\":Name[:NB_posts],\"Post Description\":description[:NB_posts],\"ALL Reactions\":ALL_Reactions[:NB_posts],\n",
    "                 'Like':Like[:NB_posts], 'Angry':Angry[:NB_posts], 'Love':Love[:NB_posts], 'Haha':Haha[:NB_posts], 'Sad':Sad[:NB_posts],'Care':Care[:NB_posts], \n",
    "                 'Wow':Wow[:NB_posts],\"Profile/Page url\":Profile_url[:NB_posts],\"Post url\":URLS[:NB_posts],\n",
    "                 \"Images Urls\":Images[:NB_posts],\"Date\":Date[:NB_posts],\"Scraping date\":Scraping_Date[:NB_posts],\n",
    "                 \"Comments\":Comments[:NB_posts],\"Who Comment id\":Who_Comment[:NB_posts]}) # Data Frame\n",
    "\n",
    "\n",
    "file_name=file_name+'.xlsx'\n",
    "w=pd.ExcelWriter(file_name, engine='xlsxwriter', options={'strings_to_formulas':False})\n",
    "df1.to_excel(w, sheet_name='Sheet1', index=False)\n",
    "w.save()\n",
    "w.close()\n",
    "\n",
    "\n",
    "print(\"data successfully saved \")\n",
    "time.sleep(1)\n",
    "print(\"--------------------------------\")       \n",
    "print(\"|     This process is done     |\")\n",
    "print(\"|     See you next time :)     |\")\n",
    "print(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
